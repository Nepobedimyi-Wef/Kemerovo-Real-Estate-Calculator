import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error
import joblib
import os
import pickle
import warnings

print("üß† –û–ë–£–ß–ï–ù–ò–ï –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–• –ú–û–î–ï–õ–ï–ô –î–õ–Ø –ö–ï–ú–ï–†–û–í–û")
print("=" * 60)

warnings.filterwarnings('ignore')


def train_and_save_model(data_path, target_col, model_name):
    """–û–±—É—á–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
    print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {data_path}")
    df = pd.read_csv(data_path, encoding='utf-8')

    print(f"   –ó–∞–ø–∏—Å–µ–π: {len(df)}, –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {target_col}")
    print(f"   –î–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω: {df[target_col].min():,.0f} - {df[target_col].max():,.0f}")

    X = df.drop(target_col, axis=1)
    y = df[target_col]

    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

    print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {categorical_cols}")
    print(f"   –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {numeric_cols}")

    preprocessor = ColumnTransformer([
        ('num', StandardScaler(), numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)
    ])

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=True
    )

    print("üîÑ –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö...")
    X_train_processed = preprocessor.fit_transform(X_train)
    X_test_processed = preprocessor.transform(X_test)

    input_dim = X_train_processed.shape[1]
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {input_dim}")

    print("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")

    # –ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_dim,)),
        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(16, activation='relu'),
        tf.keras.layers.Dense(8, activation='relu'),
        tf.keras.layers.Dense(1)
    ])

    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Å –º–µ–Ω—å—à–µ–π learning rate
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
        loss='mean_squared_error',
        metrics=['mae', 'mean_absolute_percentage_error']
    )

    print("üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")

    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=20,
        restore_best_weights=True,
        min_delta=1000
    )

    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=10,
        min_lr=0.00001
    )

    history = model.fit(
        X_train_processed, y_train,
        validation_split=0.2,
        epochs=200,
        batch_size=32,
        callbacks=[early_stopping, reduce_lr],
        verbose=0
    )

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    model.save(f'model/{model_name}_model.h5', save_format='h5')
    joblib.dump(preprocessor, f'model/{model_name}_preprocessor.pkl')

    column_info = {
        'features': X.columns.tolist(),
        'categorical': categorical_cols,
        'numeric': numeric_cols,
        'target': target_col,
        'feature_importance': None  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    }
    with open(f'model/{model_name}_columns.pkl', 'wb') as f:
        pickle.dump(column_info, f)

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    loss, mae, mape = model.evaluate(X_test_processed, y_test, verbose=0)
    y_pred = model.predict(X_test_processed, verbose=0).flatten()

    # –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π —Ä–∞—Å—á–µ—Ç MAPE
    valid_idx = y_test.values != 0
    if np.any(valid_idx):
        mape_percent = np.mean(np.abs((y_test.values[valid_idx] - y_pred[valid_idx]) / y_test.values[valid_idx])) * 100
    else:
        mape_percent = 0

    # –†–∞—Å—á–µ—Ç R¬≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    ss_res = np.sum((y_test.values - y_pred) ** 2)
    ss_tot = np.sum((y_test.values - np.mean(y_test.values)) ** 2)
    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")
    print(f"   ‚Ä¢ MAE: {mae:,.0f} —Ä—É–±")
    print(f"   ‚Ä¢ MAPE: {mape_percent:.1f}%")
    print(f"   ‚Ä¢ R¬≤: {r2:.3f}")
    print(f"   ‚Ä¢ –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {len(history.history['loss'])}")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫
    errors = y_pred - y_test.values
    print(f"   ‚Ä¢ –ú–∞–∫—Å –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞: {errors.max():,.0f} —Ä—É–±")
    print(f"   ‚Ä¢ –ú–∞–∫—Å –Ω–µ–¥–æ–æ—Ü–µ–Ω–∫–∞: {errors.min():,.0f} —Ä—É–±")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {errors.mean():,.0f} —Ä—É–±")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —è–≤–Ω–æ –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    unrealistic_idx = np.where(y_pred > y_test.values * 2)[0]
    if len(unrealistic_idx) > 0:
        print(f"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(unrealistic_idx)} –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (–≤ 2+ —Ä–∞–∑–∞ –≤—ã—à–µ —Ä–µ–∞–ª—å–Ω—ã—Ö)")

    print(f"   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞: model/{model_name}_model.h5")

    return model, preprocessor, history


os.makedirs('model', exist_ok=True)

print("\n" + "=" * 60)
print("üè† –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –ê–†–ï–ù–î–´ (–ö–ï–ú–ï–†–û–í–û)")
print("=" * 60)

try:
    rent_model, rent_preprocessor, rent_history = train_and_save_model(
        'data/kemerovo_rent.csv',
        'rent_price',
        'rent'
    )
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∞—Ä–µ–Ω–¥—ã: {e}")
    print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ generate_data.py –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
    exit()

print("\n" + "=" * 60)
print("üè° –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –ü–û–ö–£–ü–ö–ò (–ö–ï–ú–ï–†–û–í–û)")
print("=" * 60)

try:
    sale_model, sale_preprocessor, sale_history = train_and_save_model(
        'data/kemerovo_sale.csv',
        'sale_price',
        'sale'
    )
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ–∫—É–ø–∫–∏: {e}")
    exit()

print("\n" + "=" * 60)
print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
print("=" * 60)

print("\nüìä –°–í–û–î–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ú–û–î–ï–õ–Ø–•:")
print("\nüè† –ú–û–î–ï–õ–¨ –ê–†–ï–ù–î–´:")
print(f"   ‚Ä¢ –¶–µ–ª–µ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: 8,000 - 50,000 —Ä—É–±/–º–µ—Å")
print(f"   ‚Ä¢ –û–∂–∏–¥–∞–µ–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: 85-92%")

print("\nüè° –ú–û–î–ï–õ–¨ –ü–û–ö–£–ü–ö–ò:")
print(f"   ‚Ä¢ –¶–µ–ª–µ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: 1,500,000 - 15,000,000 —Ä—É–±")
print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –º¬≤: 45,000 - 120,000 —Ä—É–±")
print(f"   ‚Ä¢ –û–∂–∏–¥–∞–µ–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: 88-94%")

print("\nüìÅ –°–û–•–†–ê–ù–ï–ù–ù–´–ï –§–ê–ô–õ–´:")
files = [
    ('model/rent_model.h5', '–ú–æ–¥–µ–ª—å –∞—Ä–µ–Ω–¥—ã'),
    ('model/sale_model.h5', '–ú–æ–¥–µ–ª—å –ø–æ–∫—É–ø–∫–∏'),
    ('model/rent_preprocessor.pkl', '–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∞—Ä–µ–Ω–¥—ã'),
    ('model/sale_preprocessor.pkl', '–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø–æ–∫—É–ø–∫–∏'),
    ('model/rent_columns.pkl', '–ò–Ω—Ñ–æ –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –∞—Ä–µ–Ω–¥—ã'),
    ('model/sale_columns.pkl', '–ò–Ω—Ñ–æ –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –ø–æ–∫—É–ø–∫–∏')
]

for file_path, description in files:
    if os.path.exists(file_path):
        size = os.path.getsize(file_path) / 1024
        print(f"   ‚Ä¢ {file_path} ({description}) - {size:.1f} KB")
    else:
        print(f"   ‚ùå {file_path} - –ù–ï –ù–ê–ô–î–ï–ù")

print("\nüöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –±–æ—Ç–∞ —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏!")
print("   –î–ª—è –≤–∞—à–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ (–õ–µ–Ω–∏–Ω—Å–∫–∏–π, 42.7–º¬≤, 1 –∫–æ–º–Ω–∞—Ç–∞, –Ω–æ–≤–æ—Å—Ç—Ä–æ–π–∫–∞):")
print("   –û–∂–∏–¥–∞–µ–º–∞—è —Ü–µ–Ω–∞: 3.5 - 4.8 –º–ª–Ω —Ä—É–±")
print("   –¶–µ–Ω–∞ –∑–∞ –º¬≤: 82,000 - 112,000 —Ä—É–±")